<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Reinforcement Learning in R • rlR</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script><meta property="og:title" content="Reinforcement Learning in R">
<meta property="og:description" content="Reinforcement Learning with deep Q learning, double deep Q
    learning, frozen target deep Q learning, policy gradient deep learning, policy
    gradient with baseline deep learning, actor-critic deep reinforcement learning.">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">rlR</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-file-text-o"></span>
     
    Topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="articles/define_custom_environments.html">Specify Custom Environment</a>
    </li>
    <li>
      <a href="articles/repeated_experiment.html">Repeated Experiment</a>
    </li>
    <li>
      <a href="articles/customized_brain_mountainCar.html">Customize Neural Network Functional Approximator</a>
    </li>
    <li>
      <a href="articles/play_atari_games.html">Play Atari Games</a>
    </li>
  </ul>
</li>
<li>
  <a href="reference/index.html">
    <span class="fa fa-book"></span>
     
    Reference
  </a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/smilesun/rlR">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    

    
    

<div id="rlr-deep-reinforcement-learning-in-r" class="section level1">
<div class="page-header"><h1 class="hasAnchor">
<a href="#rlr-deep-reinforcement-learning-in-r" class="anchor"></a>rlR: Deep Reinforcement learning in R</h1></div>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<div id="r-package-installation" class="section level3">
<h3 class="hasAnchor">
<a href="#r-package-installation" class="anchor"></a>R package installation</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/reexports">install_github</a></span>(<span class="st">"smilesun/rlR"</span>)</code></pre></div>
<p>or</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/reexports">install_github</a></span>(<span class="st">"smilesun/rlR"</span>, <span class="dt">dependencies =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>rlR itself use tensorflow as its backend for neural network as functional approximator, so python dependency is needed.</p>
</div>
<div id="configure-to-connect-to-python" class="section level3">
<h3 class="hasAnchor">
<a href="#configure-to-connect-to-python" class="anchor"></a>Configure to connect to python</h3>
<p>To run the examples, you need to have the python packages <code>numpy-1.14.5</code>, <code>tensorflow-1.8.0</code>, <code>keras-2.1.6</code>, <code>gym-0.10.5</code> installed in the <strong>same</strong> python path.</p>
<p>This python path can be your system default python path or a virtual environment(either system python virtual environment or anaconda virtual environment).</p>
<p>Other package versions might also work but not tested.</p>
<p>To look at all python paths you have, in a R session, run</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reticulate<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/reticulate/topics/py_discover_config">py_discover_config</a></span>()</code></pre></div>
<p>Check which is your system default python:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">Sys.which</span>(<span class="st">"python"</span>)</code></pre></div>
<p>If you want to use a python path other than this system default, run the following(replace the ‘/usr/bin/python’ with the python path you want) before doing anything else with reticulate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reticulate<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/reticulate/topics/use_python">use_python</a></span>(<span class="st">"/usr/bin/python"</span>, <span class="dt">required=</span><span class="ot">TRUE</span>)</code></pre></div>
<p><strong>“Note that you can only load one Python interpreter per R session so the use_python call only applies before you actually initialize the interpreter.”</strong> Which means if you changed your mind, you have to close the current R session and open a new R session.</p>
<p>Confirm from the following if the first path is the one you wanted</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reticulate<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/reticulate/topics/py_config">py_config</a></span>()</code></pre></div>
</div>
<div id="python-dependencies-installation-by-rlr-function" class="section level3">
<h3 class="hasAnchor">
<a href="#python-dependencies-installation-by-rlr-function" class="anchor"></a>Python dependencies installation by rlR function</h3>
<p>It is not recommended to mix things up with the system python, so by default, the rlR facility will install the dependencies to virtual environment named ‘r-tensorflow’ either to your system virtualenv or Anaconda virtualenv.</p>
<p>For Unix user - Ensure that you have <strong>either</strong> of the following available - Python Virtual Environment:</p>
<pre><code>```bash
pip install virtualenv
```</code></pre>
<ul>
<li>Anaconda</li>
<li>Native system python that ships with your OS. (you have to install python libraries mannually in this case, see instructions below)</li>
<li>Install dependencies through</li>
<li>
<p>if you have python virtualenv available:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rlR<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/rlR/topics/installDep2SysVirtualEnv">installDep2SysVirtualEnv</a></span>(<span class="dt">gpu =</span> <span class="ot">FALSE</span>)</code></pre></div>
</li>
<li>
<p>if you have anaconda available:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rlR<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/rlR/topics/installDepConda">installDepConda</a></span>(<span class="dt">conda_path =</span> <span class="st">"auto"</span>, <span class="dt">gpu =</span> <span class="ot">FALSE</span>)</code></pre></div>
</li>
</ul>
<p>For Windows user - Ensure that you have Anaconda available <strong>or</strong> a native local system python installed(in this case you also have to install python libraries mannually, see instructions below) - Install dependencies through <code>{r eval=FALSE} rlR::installDepConda(gpu = FALSE)</code></p>
<p>If you want to have gpu support, simply set the gpu argument to be true in the function call.</p>
</div>
<div id="mannual-python-dependency-installation" class="section level3">
<h3 class="hasAnchor">
<a href="#mannual-python-dependency-installation" class="anchor"></a>Mannual python dependency installation</h3>
<p>You can also install python dependencies without using rlR facility function, for example, you can open an anaconda virtual environment “r-tensorflow” by <code>source activate r-tensorflow</code></p>
<p>All python libraries that are required could be installed either in a virtual environment or in system native python using pip:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">pip</span> install --upgrade pip  # set your prefered path to the search path first
<span class="ex">pip</span> install -r requirement.txt
<span class="co"># or</span>
<span class="ex">pip</span> install tensorflow
<span class="ex">pip</span> install keras
<span class="ex">pip</span> install gym
<span class="ex">pip</span> install cmake
<span class="ex">pip</span> install gym[atari]  # this need to be runned even you use require.txt for installation</code></pre></div>
<p>where ‘cmake’ is required to build atari environments.</p>
</div>
<div id="independencies-for-visualization-of-environments" class="section level3">
<h3 class="hasAnchor">
<a href="#independencies-for-visualization-of-environments" class="anchor"></a>Independencies for visualization of environments</h3>
<p>The R package imager is required if you want to visualize different environments but the other functionality of rlR is not affected by this R package. For ubuntu, the R package imager depends on libraries which could be installed</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> apt-get install -y libfftw3-dev libx11-dev libtiff-dev
<span class="fu">sudo</span> apt-get install -y libcairo2-dev
<span class="fu">sudo</span> apt-get install -y libxt-dev</code></pre></div>
</div>
</div>
<div id="usage" class="section level2">
<h2 class="hasAnchor">
<a href="#usage" class="anchor"></a>Usage</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rlR)</code></pre></div>
<pre><code>## system default python is /usr/bin/python</code></pre>
<pre><code>## detected available python paths are:</code></pre>
<pre><code>## python:         /home/sunxd/.virtualenvs/r-tensorflow/bin/python
## libpython:      /usr/lib/python2.7/config-x86_64-linux-gnu/libpython2.7.so
## pythonhome:     /usr:/usr
## virtualenv:     /home/sunxd/.virtualenvs/r-tensorflow/bin/activate_this.py
## version:        2.7.15rc1 (default, Apr 15 2018, 21:51:34)  [GCC 7.3.0]
## numpy:          /home/sunxd/.virtualenvs/r-tensorflow/local/lib/python2.7/site-packages/numpy
## numpy_version:  1.15.2
## 
## python versions found: 
##  /usr/bin/python
##  /usr/bin/python3
##  /home/sunxd/.virtualenvs/r-tensorflow/bin/python
##  /home/sunxd/python3virtualEnvDir/bin/python</code></pre>
<pre><code>## to set the python path you want, execute:</code></pre>
<pre><code>## [1] "reticulate::use_python('/path/to/your/python')"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/listGymEnvs.html">listGymEnvs</a></span>()[1L<span class="op">:</span>10L]</code></pre></div>
<pre><code>##  [1] "DoubleDunk-ramDeterministic-v4" "DoubleDunk-ramDeterministic-v0"
##  [3] "Robotank-ram-v0"                "CartPole-v0"                   
##  [5] "CartPole-v1"                    "Asteroids-ramDeterministic-v4" 
##  [7] "Pooyan-ram-v4"                  "Gopher-ram-v0"                 
##  [9] "HandManipulateBlock-v0"         "Pooyan-ram-v0"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="reference/makeGymEnv.html">makeGymEnv</a></span>(<span class="st">"CartPole-v1"</span>)
<span class="kw"><a href="reference/listAvailAgent.html">listAvailAgent</a></span>()</code></pre></div>
<pre><code>##                name
## 1:         AgentDQN
## 2:        AgentFDQN
## 3:        AgentDDQN
## 4:          AgentPG
## 5:  AgentPGBaseline
## 6: AgentActorCritic
## 7:        AgentDDPG
##                                                       note
## 1:                                         Deep Q learning
## 2:                           Frozen Target Deep Q Learning
## 3:                                   Double Deep QLearning
## 4:                             Policy Gradient Monte Carlo
## 5:                           Policy Gradient with Baseline
## 6:                                     Actor Critic Method
## 7: Deep Deterministic Policy Gradient for Continous Action</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env<span class="op">$</span><span class="kw">overview</span>()</code></pre></div>
<pre><code>## 
## action cnt: 2 
## state original dim: 4 
## discrete action</code></pre>
<p>If you have R package “imager” installed, you could get a snapshot of the environment by</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env<span class="op">$</span><span class="kw">snapshot</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">1000</span>)
<span class="kw"><a href="reference/listAvailConf.html">listAvailConf</a></span>()[, .(name, note, name)]</code></pre></div>
<pre><code>##                         name                                                                                                                      note                     name
##  1:                   render                                                                                    Whether to show rendering video or not                   render
##  2:                      log                                                                             Whether to log important information on drive                      log
##  3:                  console                                                                            Whether to enable debug info output to console                  console
##  4:              agent.gamma                                                                             The discount factor in reinforcement learning              agent.gamma
##  5:     agent.flag.reset.net                                                                                      Whether to reset the neural network      agent.flag.reset.net
##  6:           agent.lr.decay                                                                        The decay factor of the learning rate at each step           agent.lr.decay
##  7:                 agent.lr                                                                                               learning rate for the agent                 agent.lr
##  8:        agent.store.model                                                                            whether to store the model of the agent or not        agent.store.model
##  9: agent.update.target.freq                                                                                How often should the target network be set agent.update.target.freq
## 10:        agent.start.learn                                                                            after how many transitions should replay begin        agent.start.learn
## 11:            agent.clip.td                                                                                                  whether to clip TD error            agent.clip.td
## 12:        policy.maxEpsilon                                                                                      The maximum epsilon exploration rate        policy.maxEpsilon
## 13:        policy.minEpsilon                                                                                      The minimum epsilon exploration rate        policy.minEpsilon
## 14:        policy.decay.rate                                                                                                            the decay rate        policy.decay.rate
## 15:        policy.decay.type                                                        the way to decay epsion, can be decay_geo, decay_exp, decay_linear        policy.decay.type
## 16:       policy.aneal.steps how many steps needed to decay from maximum epsilon to minmum epsilon, only valid when policy.decay.type = 'decay_linear'       policy.aneal.steps
## 17:   policy.softmax.magnify                                                                                                                      &lt;NA&gt;   policy.softmax.magnify
## 18:         replay.batchsize                                                                     how many samples to take from replay memory each time         replay.batchsize
## 19:           replay.memname                                                                                                 The type of replay memory           replay.memname
## 20:          replay.mem.size                                                                                             The size of the replay memory          replay.mem.size
## 21:            replay.epochs                                                               How many gradient decent epochs to carry out for one replay            replay.epochs
## 22:              replay.freq                                                                                   how many steps to wait until one replay              replay.freq
##                         name                                                                                                                      note                     name</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">conf =<span class="st"> </span><span class="kw"><a href="reference/getDefaultConf.html">getDefaultConf</a></span>(<span class="st">"AgentDQN"</span>)
conf<span class="op">$</span><span class="kw">show</span>()</code></pre></div>
<pre><code>##                                      value
## render                               FALSE
## log                                  FALSE
## console                              FALSE
## agent.gamma                           0.99
## agent.flag.reset.net                  TRUE
## agent.lr.decay           0.999000499833375
## agent.lr                             0.001
## agent.store.model                    FALSE
## agent.update.target.freq               200
## agent.start.learn                       64
## agent.clip.td                        FALSE
## policy.maxEpsilon                        1
## policy.minEpsilon                     0.01
## policy.decay.rate        0.999000499833375
## policy.decay.type                decay_geo
## policy.aneal.steps                   1e+06
## policy.softmax.magnify                   1
## replay.batchsize                        64
## replay.memname                     Uniform
## replay.mem.size                      20000
## replay.epochs                            1
## replay.freq                              1
## policy.name                    ProbEpsilon</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">conf<span class="op">$</span><span class="kw">set</span>(<span class="dt">render =</span> <span class="ot">FALSE</span>, <span class="dt">console =</span> <span class="ot">FALSE</span>)   <span class="co"># Since this file is generated by Rmarkdown, we do not want other output message to blur the markdown file. In practice, you could set any configuration at once by looking at listAvailConf</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">agent =<span class="st"> </span><span class="kw"><a href="reference/initAgent.html">initAgent</a></span>(<span class="st">"AgentDQN"</span>, env, conf)
ptmi =<span class="st"> </span><span class="kw">proc.time</span>()
perf =<span class="st"> </span>agent<span class="op">$</span><span class="kw">learn</span>(200L)  
<span class="kw">proc.time</span>() <span class="op">-</span><span class="st"> </span>ptmi</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">agent<span class="op">$</span><span class="kw">plotPerf</span>()</code></pre></div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <div class="links">
<h2>Links</h2>
<ul class="list-unstyled">
<li>Browse source code at <br><a href="https://github.com/smilesun/rlR">https://​github.com/​smilesun/​rlR</a>
</li>
<li>Report a bug at <br><a href="https://github.com/smilesun/rlR/issues">https://​github.com/​smilesun/​rlR/​issues</a>
</li>
</ul>
</div>
<div class="license">
<h2>License</h2>
<ul class="list-unstyled">
<li>
<a href="https://opensource.org/licenses/BSD-2-Clause">BSD_2_clause</a> + file <a href="LICENSE-text.html">LICENSE</a>
</li>
</ul>
</div>
<div class="developers">
<h2>Developers</h2>
<ul class="list-unstyled">
<li>Xudong Sun <br><small class="roles"> Author, maintainer </small>  </li>
<li><a href="authors.html">All authors...</a></li>
</ul>
</div>

      <div class="dev-status">
<h2>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://travis-ci.com/smilesun/rlR"><img src="https://travis-ci.com/smilesun/rlR.svg?branch=master" alt="Build Status"></a></li>
<li><a href="https://coveralls.io/github/smilesun/rlR?branch=master"><img src="https://coveralls.io/repos/github/smilesun/rlR/badge.svg?branch=master" alt="Coverage Status"></a></li>
<li><a href="https://ci.appveyor.com/project/smilesun/rlr"><img src="https://ci.appveyor.com/api/projects/status/d0oyb358bh3e8r7r?svg=true" alt="Build status"></a></li>
</ul>
</div>
</div>

</div>


      <footer><div class="copyright">
  <p>Developed by Xudong Sun.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
