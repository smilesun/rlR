<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Define custom environment for deep reinforcement learn â€¢ rlR</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="Define custom environment for deep reinforcement learn">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">rlR</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-file-text-o"></span>
     
    Topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/define_custom_environments.html">Build Custom Environment</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">
    <span class="fa fa-book"></span>
     
    Reference
  </a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/smilesun/rlR">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Define custom environment for deep reinforcement learn</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/smilesun/rlR/blob/master/vignettes/define_custom_environments.Rmd"><code>vignettes/define_custom_environments.Rmd</code></a></small>
      <div class="hidden name"><code>define_custom_environments.Rmd</code></div>

    </div>

    
    
<div id="rlr-define-custom-environments-for-deep-reinforcement-learning-in-r" class="section level1">
<h1 class="hasAnchor">
<a href="#rlr-define-custom-environments-for-deep-reinforcement-learning-in-r" class="anchor"></a>rlR: Define custom environments for Deep Reinforcement learning in R</h1>
<div id="environment-class" class="section level2">
<h2 class="hasAnchor">
<a href="#environment-class" class="anchor"></a>Environment class</h2>
<p>If you want to use this package for your self defined task, you need to implement your own R6 class to represent the environment. Below is a template. Where the listed fields (member variables and methods) must exist in your implemented R6 class. You could define other public and private members as you like.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rlR)
<span class="kw">library</span>(R6)
MyEnv =<span class="st"> </span>R6::<span class="kw"><a href="http://www.rdocumentation.org/packages/R6/topics/R6Class">R6Class</a></span>(<span class="st">"MyEnv"</span>,
  <span class="dt">public =</span> <span class="kw">list</span>(
    <span class="dt">state_dim =</span> <span class="ot">NULL</span>,  <span class="co"># obligatory field, should be vector. c(28, 28, 3) which is usually the dimension for an order 3 tensor which can represent an RGB image for example.</span>
    <span class="dt">act_cnt =</span> <span class="ot">NULL</span>, <span class="co"># obligatory field, should be type integer</span>
    <span class="dt">s_r_d_info =</span> <span class="ot">NULL</span>,
    <span class="dt">step_cnt =</span> <span class="ot">NULL</span>,
    <span class="dt">initialize =</span> function(...) {
      <span class="co"># put your initialization code here for example</span>
      self$state_dim =<span class="st"> </span><span class="dv">4</span>
      self$act_cnt =<span class="st"> </span><span class="dv">2</span>
      self$step_cnt =<span class="st"> </span>0L
      self$s_r_d_info =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">"list"</span>, <span class="dt">length =</span> <span class="dv">4</span>)
      <span class="kw">names</span>(self$s_r_d_info) =<span class="st"> </span><span class="kw">c</span>(<span class="st">"state"</span>, <span class="st">"reward"</span>, <span class="st">"done"</span>, <span class="st">"info"</span>)
      self$s_r_d_info[[<span class="st">"reward"</span>]] =<span class="st"> </span><span class="dv">1</span>  <span class="co"># design your own reward scheme in step function below</span>
      self$s_r_d_info[[<span class="st">"done"</span>]] =<span class="st"> </span><span class="ot">FALSE</span>  <span class="co"># whether the episode is finished?</span>
      self$s_r_d_info[[<span class="st">"state"</span>]] =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim =</span> self$state_dim) <span class="co"># state must be array of the same dimension as the self$state_dim</span>
      self$s_r_d_info[[<span class="st">"info"</span>]] =<span class="st"> </span><span class="kw">list</span>()  <span class="co"># info can be arbitray object</span>
      as.array
    },

    <span class="dt">render =</span> function(...) {
      <span class="co"># you could leave this field empty. </span>
    },

    <span class="co"># this function will be called at each step of the learning</span>
    <span class="dt">step =</span> function(action) {
      <span class="co"># input your custom code here</span>
      self$step_cnt =<span class="st"> </span>self$step_cnt +<span class="st"> </span>1L
      self$s_r_d_info[[<span class="st">"reward"</span>]] =<span class="st"> </span><span class="kw">rnorm</span>(1L)  <span class="co"># design your own reward scheme in step function below</span>
      if(self$step_cnt &gt;<span class="st"> </span>5L)  self$s_r_d_info[[<span class="st">"done"</span>]] =<span class="st"> </span><span class="ot">TRUE</span>
      self$s_r_d_info
    },

    <span class="co"># this function will be called at the beginning of the learning and at the end of each episode</span>
    <span class="dt">reset =</span> function() {
      <span class="co"># input your custom function FALSE</span>
      self$step_cnt =<span class="st"> </span><span class="dv">0</span>
      self$s_r_d_info[[<span class="st">"done"</span>]] =<span class="st"> </span><span class="ot">FALSE</span>
      self$s_r_d_info
    },

    <span class="dt">afterAll =</span> function() {
      <span class="co"># what to do after the whole learning is finished?  could be left empty</span>
    }
    ),
  <span class="dt">private =</span> <span class="kw">list</span>(),
  <span class="dt">active =</span> <span class="kw">list</span>()
  )</code></pre>
<p>Afterwards you could choose one of the available Agents to learn on this newly defined environments.</p>
<pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span>MyEnv$<span class="kw">new</span>()
<span class="kw"><a href="../reference/listAvailAgent.html">listAvailAgent</a></span>()
## [1] "AgentDQN:deep q learning"                      
## [2] "AgentFDQN:frozen target deep q learning"       
## [3] "AgentDDQN: double deep q learning"             
## [4] "AgentPG: policy gradient basic"                
## [5] "AgentPGBaseline: policy gradient with baseline"
## [6] "AgentActorCritic: actor critic method"
agent =<span class="st"> </span><span class="kw"><a href="../reference/makeAgent.html">makeAgent</a></span>(<span class="st">"AgentDQN"</span>, env)
## parameters: 
## -render: - FALSE-
## -agent.gamma: - 0.99-
## -policy.maxEpsilon: - 1-
## -policy.minEpsilon: - 0.001-
## -policy.decay: - 0.999000499833375-
## -replay.memname: - Uniform-
## -replay.epochs: - 1-
## -interact.maxiter: - 500-
## -policy.epi_wait_ini: - 100-
## -policy.epi_wait_expl: - 20-
## -console: - FALSE-
## -log: - FALSE-
## -policy.name: - EpsilonGreedy-
## -replay.batchsize: - 64-
## -agent.nn.arch: nhidden- 64-
##  -agent.nn.arch: act1- relu-
##  -agent.nn.arch: act2- linear-
##  -agent.nn.arch: loss- mse-
##  -agent.nn.arch: lr- 0.00025-
##  -agent.nn.arch: kernel_regularizer- regularizer_l2(l=0.0)-
##  -agent.nn.arch: bias_regularizer- regularizer_l2(l=0.0)-
agent$<span class="kw">updatePara</span>(<span class="st">"console"</span>, <span class="ot">FALSE</span>)
perf =<span class="st"> </span>agent$<span class="kw">learn</span>(<span class="dv">10</span>)</code></pre>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li>
<a href="#rlr-define-custom-environments-for-deep-reinforcement-learning-in-r">rlR: Define custom environments for Deep Reinforcement learning in R</a><ul class="nav nav-pills nav-stacked">
<li><a href="#environment-class">Environment class</a></li>
      </ul>
</li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Xudong Sun.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
